\documentclass{unswmaths}

\usepackage{unswshortcuts}

\begin{document}

\subject{Measure Theory}
\author{Edward McDonald}
\title{Assignment 2}
\studentno{3375335}


\setlength\parindent{0pt}


\newcommand{\Bor}{\mathcal{B}}
\newcommand{\sdiff}{\bigtriangleup}
\newcommand{\Expect}{{\rm I\kern-.3em E}}
\newcommand{\Prob}{{\rm I\kern-.3em P}}
\newcommand{\Log}{\operatorname{Log}}


\unswtitle{}


\section*{Question 1}
For this question, let $\mu$ and $\nu$ be probability measures
    on $(\Rl^d,\Bor(\Rl^d))$.

\begin{lemma}
    The function $x\mapsto \nu(B-x)$ is $\Bor(\Rl^d)$
    measurable for any $B \in \Bor(\Rl^d)$.
\end{lemma} 
\begin{proof}
    Let $B \in \Bor(\Rl^d)$ and $s(x) = \nu(B-x)$. Then,
    \begin{align*}
        s(x) &= \int_{\Rl^d} \chi_{B-x}\;d\nu\\
        &= \int_{\Rl^d} \chi_B(y+x)\;d\nu(y).
    \end{align*}
    The function $(x,y)\mapsto \chi_{B}(y+x)$
    is a composition of a continuous function, $(x,y)\mapsto y+x$
    and a measurable function $x\mapsto \chi_B(x)$. 
    Hence the function $(x,y)\mapsto \chi_{B}(y+x)$
    is $\Bor(\Rl^d)\otimes\Bor(\Rl^d)$ measurable,
    and so by Tonelli's theorem $s$ is $\Bor(\Rl^d)$
    measurable.
\end{proof}

\begin{lemma}
    The convolution measure $\mu\star\nu(B) = \int_{\Rl^d} \nu(B-x)\;d\mu(x)$
    is well defined and finite.
\end{lemma}
\begin{proof}
    Since $\mu\star\nu(B)$ is defined as an integral of a positive
    measurable function, the integral exists. Since $\nu(B-x) \leq 1$,
    we have $\mu\star\nu(B) \leq 1$.
\end{proof}

\begin{theorem}
    If there exists some bounded $F$
    such that $\mu\star\nu(F) = 1$, then there are bounded
    sets $G$ and $H$ such that $\mu(G) = 1$ and $\nu(H) = 1$.
    Similar results hold where ``bounded" is replaced with
    ``finite" or ``countable".
\end{theorem}
\begin{proof}
    Suppose that there exists a bounded set $F \in \Bor(\Rl^d)$
    such that $\mu\star\nu(F) = 1$, but for any bounded
    set $B$, $\mu(B),\nu(B) < 1$. Then,
    \begin{align*}
        \mu\star\nu(F) &= \int \nu(F-x) \;d\mu(x)\\
        &< \int 1\;d\mu\\
        &= 1.
    \end{align*}
    since $F-x$ is bounded for any $x$. This is a contradiction.
    Hence we must have
    that $\nu$ attains the value $1$ on some bounded set.
    
    By symmetry, $\mu$ must also take the value $1$ on some bounded set.
    
    An identical argument holds if ``bounded" is replaced by ``finite" or
    ``countable".
\end{proof}

\section*{Question 2}
For this question, $\mu$ and $\nu$ are $\sigma$-finite positive measures
on a measurable space $(\Omega,\mathcal{F})$.
\begin{theorem}
    The following are equivalent:
    \begin{enumerate}
        \item{} $\mu$ and $\nu$ have exactly the same null sets.
        \item{} $\mu \ll \nu$ and $\nu \ll \mu$.
        \item{} There is an $\mathcal{F}$-measurable
        function $g$ with $0 < g < +\infty$ such that $\nu(A) = \int_A g\;d\mu$
        for all $A \in \mathcal{F}$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    First we prove $(1)\Rightarrow(2)$.
    
    Assume that $\mu$ and $\nu$ have exactly the same null sets. 
    Then if $\mu(A) = 0$, then $\nu(A) = 0$.
    That is $\nu \ll \mu$. Similarly, $\mu \ll \nu$.
    
    Now we prove $(2)\Rightarrow(3)$. 
    
    Assume that $\nu \ll \mu$. 
    
    By the Radon-Nikodym Theorem,
    there is an $\mathcal{F}$-measurable function $g$
    such that $\nu(A) = \lambda(A)+\int_A g\;d\mu$,
    where $\lambda$ is a measure mutually singular to $\mu$. Suppose
    that $\mu(A) = 0$, then $\nu(A) = 0$ by assumption, hence $\lambda(A) = 0$.
    Thus $\lambda \ll \mu$ and $\lambda \bot \mu$, so $\lambda = 0$.
    
    Suppose that $g = \infty$ only on a null set. Then we can modify
    $g$ to $g = 0$ on this set since $g$ is determined only up
    to $\mu$-almost everywhere equivalence.
    
    Suppose there is some set $A$ with $\mu(A) > 0$ and $g(A) = \{\infty\}$.
    
    Since $\nu$ is $\sigma$-finite, there exists $B$ with $\nu(B) < \infty$
    and $\nu(A\cap B) > 0$.
    
    Hence $g(A\cap B) = \{\infty\}$, and we cannot have $\mu(A\cap B) = 0$
    because then $\nu(A\cap B) = 0$.
    
    But then $\nu(A\cap B) = \infty$, but this is a contradiction.
    Hence $g < \infty$ $\mu$-almost everywhere.
    
    Now suppose there is some set $C$ with $\mu(C) > 0$
    and $g(C) = \{0\}$. Hence $\nu(C) = 0$, but since $\mu \ll \nu$
    this is a contradiction.
    
    Now we prove that $(3)\Rightarrow(1)$.
    
    Suppose that $\mu(A) = 0$. Then clearly since $\nu(A) = \int_A g\;d\mu(A)$,
    we have $\nu(A) = 0$.
    
    Now suppose that $\nu(A) = 0$. Now,
    \begin{equation*}
        \nu(A) \geq \frac{1}{n}\mu(A\cap g^{-1}([1/n,\infty))
    \end{equation*}
    Hence $\mu(A\cap g^{-1}([1/n,\infty)) = 0$. But since $g > 0$, we
    have 
    \begin{equation*}
        \mu(A) = \lim_{n\rightarrow\infty} \mu(A\cap g^{-1}([1/n,\infty)).
    \end{equation*}
    Hence $\mu(A) = 0$. Thus $\mu$ and $\nu$ have the same null 
    sets so $(1)$ is proved.
\end{proof}

\begin{theorem}
    Suppose that $\mu$ is a $\sigma$-finite positive measure on $(\Omega,\mathcal{F})$. Then there is a finite positive measure $\nu$
    on $(\Omega,\mathcal{F})$ such that $\nu \ll \mu$ and $\mu \ll \nu$.
\end{theorem}
\begin{proof}
    Suppose that $\{A_k\}_{k=1}^\infty$ is a disjoint sequence of sets
    with $A = \bigcup_k A_k$ and $0 < \mu(A_k) < \infty$. This can be chosen
    since $\mu$ is $\sigma$-finite.
    
    Now define,
    \begin{equation*}
        \nu(A) = \sum_{k=1}^\infty \frac{1}{2^k \mu(A_k)}\mu(A\cap A_k)
    \end{equation*}
    for $A \in \mathcal{F}$.
    This sum converges since each term is bounded by $1/2^k$, so the sum
    is bounded by a geometric series.
    
    We wish to show that $\nu$ is a probability measure
    on $(\Omega,\mathcal{F})$ and than $\mu \ll \nu$ and $\nu \ll \mu$. 
    
    Note that $\nu(\Omega) = 1$ and $\nu(\emptyset) = 0$.
    
    Suppose that $\{B_k\}_{k=1}^\infty$ is a disjoint sequence of sets
    in $\mathcal{F}$. Then
    \begin{equation*}
        \nu(\bigcup_k B_k) = \sum_{j=1}^\infty \sum_{k=1}^\infty \frac{1}{2^j \mu(A_j)}\mu(B_k\cap A_j)
    \end{equation*}
    Then since this is a sum of positive numbers, we can change the order
    of summation by Tonelli's theorem,
    \begin{equation*}
        \nu(\bigcup_k B_k) = \sum_{k=1}^\infty \nu(B_k).
    \end{equation*}
    Hence $\nu$ is countably additive, so is a measure on $(\Omega,\mathcal{F})$.
    
    Now we wish to show that $\nu \ll \mu$. Suppose that $\mu(A) = 0$.
    Then clearly $\mu(A_k \cap A) = 0$ for all $k$, so $\nu(A) = 0$. 
    Thus $\nu \ll \mu$.
    
    Now to show that $\mu \ll \nu$, let  
    $\nu(A) = 0$, then $\mu(A_k \cap A) = 0$ for all $k$. Hence,
    \begin{equation*}
        \mu(A) = \sum_{k=1}^\infty \mu(A_k\cap A) = 0.
    \end{equation*}
    Thus $\mu \ll \nu$. 
\end{proof}

\section*{Question 3}
For this question, $X$ is a $d$-dimensional random vector with law
$\mu$ and characteristic function $\hat{\mu}(u)$.
\begin{lemma}
    The characteristic function of $cX$ is $\hat{\mu}(cu)$,
    for any $c \in \Rl$.
\end{lemma} 
\begin{proof}
    This is a simple computation. By definition, the characteristic
    function of $cX$ is
    \begin{equation*}
        \Expect(e^{i\langle u,cX\rangle}
    \end{equation*}
    But since $\langle u,cX\rangle = \langle cu,X\rangle$, this is simply
    $\hat{\mu}(cu)$.
\end{proof}

\begin{theorem}
    If $X$ has moments up to order $n$, then $\hat{\mu}$ is differentiable
    at $0$ up to $n$th order, and $\frac{\partial^\alpha}{\partial u^\alpha}\hat{\mu}(u)|_{u=0} = i^{|\alpha|}\Expect(X^\alpha)$ for multi-indices $\alpha$, with
    $|\alpha| \leq n$.
\end{theorem}
\begin{proof}
    
    Suppose that $X$ has moments up to order $n$. Then we can say that,
    \begin{equation*}
        \frac{\partial}{\partial u_j} \Expect(e^{i\langle u,X\rangle}) = i\Expect(X_je^{i\langle u,X\rangle})
    \end{equation*}
    by the Dominated convergence theorem, since $\Expect(|X_j|) < \infty$. Hence,
    by induction,
    \begin{equation*}
        \frac{\partial^\alpha}{\partial u^\alpha}\Expect(e^{i\langle u,X\rangle}) = i^{|\alpha|} \Expect(X^\alpha e^{i\langle u,X\rangle}).
    \end{equation*}
    for all multi-indices $\alpha$ with $|\alpha| \leq n$ by the Dominated
    convergence theorem. This shows that the derivative exists.
    
    So we simply evaluate this at zero to obtain the required result.
\end{proof}    
Now we let $X$ be a random variable with Lebesgue density
\begin{equation*}
    f(x) = \frac{C}{(1+x^2)\log(e+x^2)}
\end{equation*}
for some normalising constant $C > 0$. Let $\hat{\mu}$ be the
characteristic function of $X$.

\begin{lemma}
    $\Expect(X)$ is not defined. That is, $X$ does not have moments up
    to order $1$.
\end{lemma}
\begin{proof}
    Note that $\log(e+x^2) $
\end{proof}
    
\begin{lemma}
    $\hat{\mu}$ is differentiable at $0$.
\end{lemma}
\begin{proof}
    
\end{proof}

\section*{Question 4}
For this question, $\mu$ is the binomial distribution $\mathrm{Bin}(n,p)$,
and $\nu$ is the Poisson distribution with mean $\lambda > 0$.

\begin{lemma}
    The characteristic function of a Bernoulli random variable
    with probability $p$ is
    \begin{equation*}
        1-p+pe^{iu}
    \end{equation*}
\end{lemma}
\begin{proof}
    This is a simple computation, if $X$ takes the value $1$ with probability
    $p$ and $0$ with probability $1-p$, then
    \begin{equation*}
        \Expect(e^{iuX}) = 1-p+pe^{iu}.
    \end{equation*}
\end{proof}
\begin{lemma}
    $\hat{\mu}(u) = (1-p+pe^{iu})^n$.
\end{lemma}
\begin{proof}
    A sum of $n$ independent Bernoulli random variables with
    probability $p$ is $\mathrm{Bin}(n,p)$ distributed. So,
    \begin{equation*}
        {\mu} = {\mathrm{Bern}(p)}^{\star n}
    \end{equation*}
    
    Hence,
    \begin{equation*}
        \hat{\mu}(u) = (1-p+pe^{iu})^n.
    \end{equation*}
\end{proof}



\begin{lemma}
    $\hat{\nu}(u) = \exp(\lambda(e^{iu}-1))$.
\end{lemma}
\begin{proof}
    We can compute,
    \begin{equation*}
        \hat{\nu}(u) = \sum_{k=0}^\infty \frac{e^{iuk}e^{-\lambda}\lambda^k}{k!}
    \end{equation*}
    So we write this as,
    \begin{align*}
        \hat{\nu}(u) &= \sum_{k=0}^\infty e^{-\lambda} \frac{(e^{iu}\lambda)^k}{k!}\\
        &= e^{-\lambda} \exp(\lambda e^{iu})\\
        &= \exp(\lambda(e^{iu}-1)).
    \end{align*}
\end{proof}

\begin{lemma}
\label{stuart}
    Suppose that $q_n \rightarrow \lambda \in \Cplx$ is a sequence of complex
    numbers. Then
    \begin{equation*}
        \lim_{n\rightarrow\infty} \left(1+\frac{q_n}{n}\right)^n = e^{\lambda}
    \end{equation*} 
\end{lemma}
\begin{proof}
    Fix $n$ large enough such that $|q_n|/n < 1/2$.
    
    Since $q_n$ is a convergent sequence, it is bounded. Let $M$
    be large enough such that $|q_n| < M$ for all $n$.
    
    Re-write $\left(1+\frac{q_n}{n}\right)^n$ as $\exp(n\Log(1+\frac{q_n}{n}))$.
    
    The branch of the logarithm taken here is complex differentiable
    in the set $\Cplx\setminus(-\infty,0]$. Since $|q_n|/n < 1$, the above is valid.
    
    So it is sufficient to show that,
    \begin{equation*}
        \lim_{n\rightarrow\infty} n\Log\left(1+\frac{q_n}{n}\right) = \lambda
    \end{equation*}
    
    The $z\mapsto \Log(1+z)$ function is complex differentiable in the unit disc,
    and has a power series representation 
    \begin{equation*}
        \Log(1+z) = \sum_{k=1}^\infty (-1)^{k-1}\frac{z^k}{k}
    \end{equation*}
    which converges uniformly on compact subsets of the open unit disc $\{z\in \Cplx\;:\;|z| < 1\}$.
    
    Now, since $|q_n|/n < 1$, we have
    \begin{equation*}
        n\Log(1+\frac{q_n}{n}) = q_n+\sum_{k=2}^\infty (-1)^{k-1}\frac{q_n^k}{kn^{k-1}}
    \end{equation*}
    
    Now we consider the tail of the left hand side, let
    \begin{equation*}
        L_n := \sum_{k=2}^\infty (-1)^{k-1} \frac{q_n^k}{kn^{k-1}}
    \end{equation*}
    
    By the triangle inequality,
    \begin{equation*}
        |L_n| \leq \sum_{k=2}^\infty \frac{M^k}{kn^{k-1}}
    \end{equation*}
    Thus,
    \begin{align*}
        |L_n| &\leq M\sum_{k=1}^\infty \left(\frac{M}{n}\right)^k\\
        &= M\frac{M/n}{(1-M/n)}
    \end{align*}
    Hence, $L_n\rightarrow 0$ as $n\rightarrow\infty$.
    Thus, the limit
    \begin{equation*}
        \lim_{n\rightarrow\infty} n\Log(1+\frac{q_n}{n})
    \end{equation*}
    exists, and equals $\lim_{n\rightarrow\infty}q_n = \lambda$.
    
    Hence, the limit
    \begin{equation*}
        \lim_{n\rightarrow\infty}\left(1+\frac{q_n}{n}\right)^n
    \end{equation*}
    exists, and equals $e^\lambda$.
\end{proof}

Now we let $\{p_n\}_{n=1}^\infty$ be a monotone decreasing sequence, such
that $np_n \rightarrow \lambda$. We let $\mu_n = \mathrm{Bin}(n,p_n)$.
\begin{theorem}
    There is weak convergence, $\mu_n\rightarrow\nu$.
\end{theorem}
\begin{proof}
    By L\'evy's continuity theorem, it is sufficient to show pointwise convergence
    of characteristic functions, $\hat{\mu}_n(u)\rightarrow \hat{\nu}(u)$
    for all $u$. That is, we must show
    \begin{equation*}
        \lim_{n\rightarrow\infty} (1-p_n+p_ne^{iu})^n = \exp(\lambda(e^{iu}-1)).
    \end{equation*}    
    
    Rewrite $\hat{\mu}_n(u)$ as
    \begin{equation*}
        \left(1+\frac{np_n(e^{iu}-1)}{n}\right)^n
    \end{equation*}
    Now by lemma \ref{stuart}, we see
    \begin{equation*}
        \lim_{n\rightarrow\infty} \hat{\mu}_n(u) = \exp(\lambda(e^{iu}-1)).
    \end{equation*}
    Thus the result follows.
\end{proof}

\begin{theorem}
    For $k \in \Ntrl$, we have the pointwise convergence
    \begin{equation*}
        \mu_n(\{k\})\rightarrow \nu(\{k\}).
    \end{equation*}
\end{theorem}
\begin{proof}
    Let $F_n(x) = \mu_n(-\infty,x]$ be the cumulative
    distribution function of $\mu_n$, and $G(x) = \nu(-\infty,x]$
    
    Weak convergence implies that $F_n(x)\rightarrow G(x)$
    at all points of continuity $x$. $\mu_n$
    and $\nu$ are discrete, so the points of continuity are
    $\Rl\setminus \Ntrl$. 
    
    Now let $k \in \Ntrl$. Then
    \begin{align*}
        \lim_{n\rightarrow\infty} \mu_n\{k\} &= \lim_{n\rightarrow\infty} F_n(k+1/2)-F_n(k-1/2)\\
        &= G(k+1/2)-G(k-1/2)\\
        &= \nu\{k\}.
    \end{align*}
\end{proof}

\section*{Question 5}
Now we let $(\Omega,\mathcal{F},\Prob) = ([0,1],\Bor([0,1]),\lambda)$
where $\lambda$ is Lebesgue measure.

For $\omega \in [0,1]$, we let $d_n(\omega)$ be the $n$ binary digit
of $\omega$, where we take the binary expansion containing infinitely many ones.
Let 
\begin{equation*}
    B_n = \{\omega \in [0,1]\;:\;d_n(\omega) = 0\}.
\end{equation*}

\begin{lemma}
    $\Prob(B_n) = 1/2.$
\end{lemma}
\begin{proof}
    See that,
    \begin{equation*}
        B_n = \bigcup_{k=0}^{2^{n-1}}k/2^{n-1}+[0,2^{-n}]
    \end{equation*}
    So $B_n \in \Bor([0,1])$.


    See that $B_n^c = \{\omega \in [0,1]\;:\;d_n(\omega) = 1\}$.
    
    So that $B_n^c = B_n+2^{-n}$. Since Lebesgue measure
    is translation invariant, we have $\Prob(B_n^c) = \Prob(B_n)$.
    Since $\Prob(\Omega) = \Prob(B_n)+\Prob(B_n^c) = 1$,
    we have $\Prob(B_n) = 1/2$.
\end{proof}

\begin{lemma}
    The events $\{B_n\}_{n=1}^\infty$ form an infinite sequence of independent
    events.
\end{lemma}
\begin{proof}
    Suppose we have some finite subset, $\{B_{n(k)}\}_{k=1}^m$. Then let
    \begin{equation*}
        B = \bigcap_{k=1}^m B_{n(k)}
    \end{equation*}
%    We may choose the subset $\{n(k)\}_{k=1}^m$ so be an increasing
%    set of numbers.
    See that,
    \begin{equation*}
        \bigcap_{k=2}^m B_{n(k)} = B_{n(1)}\cap\bigcap_{k=2}^m B_{n(k)}\cup B_{n(1)}^c\cap\bigcap_{k=2}^m B_{n(k)}.
    \end{equation*}
    
    But $B_{n(1)}^c = B_{n(1)}+2^{-n(1)}$.
    
    Hence, 
    \begin{equation*}
        \Prob\left(\bigcap_{k=2}^m B_{n(k)}\right) = 2\Prob\left(\bigcap_{k=1}^m B_{n(k)}\right).
    \end{equation*}
    
    So by induction,
    \begin{equation*}
        \Prob\left(\bigcap_{k=2}^m B_{n(k)} \right) = 2^-m = \prod_{k=1}^m \Prob(B_{n(k)}).
    \end{equation*}
    
    Hence the sequence $\{B_n\}_{n=1}^\infty$ is independent.
\end{proof}

\begin{remark}
    Precisely the same arguments would work if we replaced
    binary digits with decimal digits.
\end{remark}

\begin{theorem}
    Given any finite sequence of digits, the probability that a
    randomly sampled number in $[0,1]$
    contained that sequence infinitely many times is $1$.
\end{theorem}
\begin{proof}
    Suppose that the sequence has length $L$. Let $E_n$
    be the event that the sequence occurs in
    the $nL$th position. Then the $E_n$ form an independent sequence
    of independent events, each with probability $10^{-L}$. Then since
    \begin{equation*}
        \sum_{n=1}^\infty \Prob(E_n) = \infty.
    \end{equation*}
    Hence, by the Borel-Cantelli lemma, the probability that infinitely
    many of the $E_n$ occur is $1$.
\end{proof}

\section*{Question 6}

\begin{lemma}
\label{firstFunctional}
    Let $s$ be a continuous solution to the functional equation
    \begin{equation*}
        4s(2x) = 3s(x)+s(-x).
    \end{equation*}
    for $x$ in a neighbourhood of $0$.
    Then $s$ is constant.
\end{lemma}
\begin{proof}
    For any constant $c$, if $s$ is a solution to the functional
    equation then so is $s+c$. So we may assume without loss
    of generality that $s(0) = 0$.
    
    Suppose $s(x) > \varepsilon > 0$. Then 
    \begin{equation*}
        3s(x/2)+s(-x/2) > 4\varepsilon. 
    \end{equation*}
    Hence since the average of $(s(x/2),s(x/2),s(x/2),s(-x/2))$
    is larger than $\varepsilon$, at least one of these numbers
    must exceed $\varepsilon$
    Hence at least one of $s(x/2),s(-x/2)$ exceeds varepsilon. 
    
    Thus we have a sequence of numbers approaching $0$ which all
    exceeds $\varepsilon$. But this contradicts $s(0) = 0$
    and continuity.
\end{proof}

\begin{lemma}
\label{secondFunctional}
    Suppose that $f$ is a solution of
    the functional equation
    \begin{equation*}
        f(2x) = f(x)^3f(-x)
    \end{equation*} 
    for $x$ in a neighbourhood of $0$, 
    with $f(0) = 1$, and $f$ is assumed to be twice
    continuously
    differentiable in a neighbourhood of
    the origin.
    
    Then $f(x) = \exp(Ax^2+Bx)$ for parameters $A$ and $B$.
\end{lemma} 
\begin{proof}
    Since $f(0) = 1$, and $f$ is continuous, then we may restrict
    $x$ sufficiently small such that $f(x) > 0$. Hence $L(x) := \log(f(x))$
    is well defined, and
    \begin{equation*}
        L(2x) = 3L(x)+L(-x).
    \end{equation*}
    Differentiating twice, we have
    \begin{equation*}
        4L''(2x) = 3L''(x)+L''(-x)
    \end{equation*}
    But by lemma \ref{firstFunctional}, we have $L''(x)$
    is constant. 
    
    Thus, $L$ is a quadratic, so $f(x) = \exp(Ax^2+Bx+C)$
    for constants $A$,$B$ and $C$. We see $C=0$
    since $f(0) = 1$.    
\end{proof}

\begin{theorem}
    Suppose that $X$ and $Y$ are independent identically distributed random
    variables, with finite variances.
    
    Also assume that $X+Y$ and $X-Y$ are independent.
    
    Then $X$ and $Y$ are Gaussian.
\end{theorem}
\begin{proof}
    Let $\varphi(u)$ be the characteristic function of $X$ and $Y$.
    Then the characteristic function of $X+Y$ is $\varphi(u)^2$,
    and the characteristic function of $X-Y$ is $\varphi(u)\varphi(-u)$.
    The characteristic function of $2X$ is $\varphi(2u)$. Since
    $2X = X+Y+X-Y$, and $X+Y$ and $X-Y$ are independent, we have
    \begin{equation*}
        \varphi(2u) = \varphi(u)^3\varphi(-u)
    \end{equation*}
    and $\varphi(0) = 1$. Since $X$ and $Y$
    have finite variances, $\varphi$ is twice continuously
    differentiable in a neighbourhood of $0$. Thus,
    by lemma \ref{secondFunctional} we have $\varphi(u) = \exp(Au^2+Bu)$
    for some parameters $A$ and $B$.
    
    This is the characteristic function of a Gaussian random variable.
\end{proof} 

\end{document}
